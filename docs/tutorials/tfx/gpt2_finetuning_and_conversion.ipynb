{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "iwgnKVaUuozP"
      ],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtDTm6wbIbpy"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");"
      ],
      "metadata": {
        "id": "iwgnKVaUuozP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBFkQLk1In7I"
      },
      "outputs": [],
      "source": [
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf3QpfdiIl7O"
      },
      "source": [
        "# TFX Pipeline for Fine-Tuning and Converting an Large Language Model (LLM)\n",
        "\n",
        "\n",
        "In this codelab, we use  KerasNLP to load a pre-trained Large Language Model (LLM) - GPT-2 model and finetune it to the Multi-News dataset. The dataset that is used in this demo is [Multi-News dataset](https://www.tensorflow.org/datasets/catalog/multi_news). After producing a finetuned model, the pipeline converts the model into a TFLite model, a format that can run on mobile devices, and a quantized TFLite model, which is a more efficient version of a TFLite model that represents the model's weights and activations using fewer bits.\n",
        "\n",
        "Note: We recommend running this tutorial in a Colab notebook, with no setup required!  Just click \"Run in Google Colab\".\n",
        "\n",
        "\u003cdiv class=\"devsite-table-wrapper\"\u003e\u003ctable class=\"tfo-notebook-buttons\" align=\"left\"\u003e\n",
        "\u003ctd\u003e\u003ca target=\"_blank\" href=\"https://www.tensorflow.org/tfx/tutorials/tfx/gpt2_finetuning_and_conversion\"\u003e\n",
        "\u003cimg src=\"https://www.tensorflow.org/images/tf_logo_32px.png\"/\u003eView on TensorFlow.org\u003c/a\u003e\u003c/td\u003e\n",
        "\u003ctd\u003e\u003ca target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tfx/blob/master/docs/tutorials/tfx/gpt2_finetuning_and_conversion.ipynb\"\u003e\n",
        "\u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\"\u003eRun in Google Colab\u003c/a\u003e\u003c/td\u003e\n",
        "\u003ctd\u003e\u003ca target=\"_blank\" href=\"https://github.com/tensorflow/tfx/tree/master/docs/tutorials/tfx/gpt2_finetuning_and_conversion.ipynb\"\u003e\n",
        "\u003cimg width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\"\u003eView source on GitHub\u003c/a\u003e\u003c/td\u003e\n",
        "\u003ctd\u003e\u003ca href=\"https://storage.googleapis.com/tensorflow_docs/tfx/docs/tutorials/tfx/gpt2_finetuning_and_conversion.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/download_logo_32px.png\" /\u003eDownload notebook\u003c/a\u003e\u003c/td\u003e\n",
        "\u003c/table\u003e\u003c/div\u003e\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU9YYythm0dx"
      },
      "source": [
        "### Why is this pipeline useful?\n",
        "TFX pipelines are valuable tools for building and managing machine learning workflows in a production environment. They are more reproducible, scalable, and modular than simply running pure Python code. This makes them ideal for working with large language models, which require many complex steps that can be difficult to organize. TFX pipelines can also help you automate the finetuning and conversion process, which can save you a lot of time and effort.  \n",
        "\n",
        "In addition, TFX pipelines can also help you track the lineage of your data and models using metadata. This means that you can easily see how your data and models were created and how they relate to each other. This can be helpful for debugging, auditing, and understanding the performance of your models.\n",
        "\n",
        "### Note\n",
        "*Note that GPT-2 is used here only to demonstrate the end-to-end process; the techniques and tooling introduced in this codelab are potentially transferrable to other generative language models such as Google T5.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WgJ8Z8gJB0s"
      },
      "source": [
        "## Before You Begin\n",
        "\n",
        "Colab offers different kinds of runtimes. Make sure to go to **Runtime -\u003e Change runtime type** and choose the GPU Hardware Accelerator runtime since you will finetune the GPT-2 model. **This tutorial needs Colab pro subscription to use 40GB of A100 GPU for execution. GPUs with less memory may result in GPU going OOM while model finetuning.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sj3HvNcJEgC"
      },
      "source": [
        "## Set Up\n",
        "\n",
        "We first install required python packages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73c9sPckJFSi"
      },
      "source": [
        "### Upgrade Pip\n",
        "To avoid upgrading Pip in a system when running locally, check to make sure that we are running in Colab. Local systems can of course be upgraded separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45pIxa6afWOf",
        "tags": []
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import colab\n",
        "  !pip install --upgrade pip\n",
        "\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIf40NdqJLAH"
      },
      "source": [
        "### Install TFX, KerasNLP and required Libraries\n",
        "\n",
        "We will be working with keras-nlp IO-2023 module in this notebook. Updating to latest keras-nlp version results in pipeline breakage because of [keras-nlp/1090](https://github.com/keras-team/keras-nlp/issues/1090). Users can upgrade to the latest keras-nlp after this issue is fixed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6mBN4dzfct7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/keras-team/keras-nlp.git@google-io-2023\\\n",
        "tfx tensorflow-text more_itertools tensorflow_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjcN5nixJVUW"
      },
      "source": [
        "### Install \"shapely\u003c2\"\n",
        "\n",
        "This is a temporary solution to avoid an ImportError while importing TFX.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFucUjUmfm3z",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install -U google-cloud-aiplatform \"shapely\u003c2\" -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0tnFDm6JRq_",
        "tags": []
      },
      "source": [
        "## Did you restart the runtime?\n",
        "\n",
        "If you are using Google Colab, the first time that you run the cell above, you must restart the runtime by clicking above \"RESTART SESSION\" button or using `\"Runtime \u003e Restart session\"` menu. This is because of the way that Colab loads packages.\n",
        "\n",
        "*Let's check the TensorFlow and TFX library versions.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hf5FbRzcfpMg",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print('TensorFlow version: {}'.format(tf.__version__))\n",
        "from tfx import v1 as tfx\n",
        "print('TFX version: {}'.format(tfx.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng1a9cCAtepl"
      },
      "source": [
        "## Installing Interactive Context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7ikXCc7v7Rh"
      },
      "source": [
        "An interactive context is used to provide global context when running a TFX pipeline in a notebook without using a runner or orchestrator such as Apache Airflow or Kubeflow. This style of development is only useful when developing the code for a pipeline, and cannot currently be used to deploy a working pipeline to production."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEge2nYDfwaM",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
        "context = InteractiveContext()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF6Kk3MLxxCC"
      },
      "source": [
        "## Pipeline Overview\n",
        "\n",
        "Below are the components that this pipeline follows.\n",
        "\n",
        "* Custom Artifacts are artifacts that we have created for this pipeline. **Artifacts** are data that is produced by a component or consumed by a component. Artifacts are stored in a system for managing the storage and versioning of artifacts called MLMD.\n",
        "\n",
        "* **Components** are defined as the implementation of an ML task that you can use as a step in your pipeline\n",
        "* Aside from artifacts, **Parameters** are passed into the components to specify an argument.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIBO-ueGVVHa"
      },
      "source": [
        "## ExampleGen\n",
        "We create a custom ExampleGen component which we use to load a TensorFlow Datasets (TFDS) dataset. This uses a custom executor in a FileBasedExampleGen.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgvIaoAmXFVp",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from typing import Any, Dict, List, Text\n",
        "import tensorflow_datasets as tfds\n",
        "import apache_beam as beam\n",
        "import json\n",
        "from tfx.components.example_gen.base_example_gen_executor import BaseExampleGenExecutor\n",
        "from tfx.components.example_gen.component import FileBasedExampleGen\n",
        "from tfx.components.example_gen import utils\n",
        "from tfx.dsl.components.base import executor_spec\n",
        "import os\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cjd9Z6SpVRCE",
        "tags": []
      },
      "outputs": [],
      "source": [
        "@beam.ptransform_fn\n",
        "@beam.typehints.with_input_types(beam.Pipeline)\n",
        "@beam.typehints.with_output_types(tf.train.Example)\n",
        "def _TFDatasetToExample(\n",
        "    pipeline: beam.Pipeline,\n",
        "    exec_properties: Dict[str, Any],\n",
        "    split_pattern: str\n",
        "    ) -\u003e beam.pvalue.PCollection:\n",
        "    \"\"\"Read a TensorFlow Dataset and create tf.Examples\"\"\"\n",
        "    custom_config = json.loads(exec_properties['custom_config'])\n",
        "    dataset_name = custom_config['dataset']\n",
        "    split_name = custom_config['split']\n",
        "\n",
        "    builder = tfds.builder(dataset_name)\n",
        "    builder.download_and_prepare()\n",
        "\n",
        "    return (pipeline\n",
        "            | 'MakeExamples' \u003e\u003e tfds.beam.ReadFromTFDS(builder, split=split_name)\n",
        "            | 'AsNumpy' \u003e\u003e beam.Map(tfds.as_numpy)\n",
        "            | 'ToDict' \u003e\u003e beam.Map(dict)\n",
        "            | 'ToTFExample' \u003e\u003e beam.Map(utils.dict_to_example)\n",
        "            )\n",
        "\n",
        "class TFDSExecutor(BaseExampleGenExecutor):\n",
        "  def GetInputSourceToExamplePTransform(self) -\u003e beam.PTransform:\n",
        "    \"\"\"Returns PTransform for TF Dataset to TF examples.\"\"\"\n",
        "    return _TFDatasetToExample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D159hAzJgK2"
      },
      "source": [
        "For example demonstration, we are using 20% of multi_news dataset. We can play around with \"custom_config\" to take in more data or the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNDu1ECBXuvI",
        "tags": []
      },
      "outputs": [],
      "source": [
        "example_gen = FileBasedExampleGen(\n",
        "    input_base='dummy',\n",
        "    custom_config={'dataset':'multi_news', 'split':'train[:20%]'},\n",
        "    custom_executor_spec=executor_spec.BeamExecutorSpec(TFDSExecutor))\n",
        "context.run(example_gen, enable_cache=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74JGpvIgJgK2"
      },
      "source": [
        "We create a convenience utility to inspect datasets of TFExamples. The ratings dataset returns a dictionary of document and summary columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GA8VMXKogXxB",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def inspect_examples(component,\n",
        "                     channel_name='examples',\n",
        "                     split_name='train',\n",
        "                     num_examples=1):\n",
        "  # Get the URI of the output artifact, which is a directory\n",
        "  full_split_name = 'Split-{}'.format(split_name)\n",
        "  print('channel_name: {}, split_name: {} (\\\"{}\\\"), num_examples: {}\\n'.format(\n",
        "      channel_name, split_name, full_split_name, num_examples))\n",
        "  train_uri = os.path.join(\n",
        "      component.outputs[channel_name].get()[0].uri, full_split_name)\n",
        "  print('train_uri: {}'.format(train_uri))\n",
        "\n",
        "  # Get the list of files in this directory (all compressed TFRecord files)\n",
        "  tfrecord_filenames = [os.path.join(train_uri, name)\n",
        "                        for name in os.listdir(train_uri)]\n",
        "\n",
        "  # Create a `TFRecordDataset` to read these files\n",
        "  dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
        "\n",
        "  # Iterate over the records and print them\n",
        "  print()\n",
        "  for tfrecord in dataset.take(num_examples):\n",
        "    serialized_example = tfrecord.numpy()\n",
        "    example = tf.train.Example()\n",
        "    example.ParseFromString(serialized_example)\n",
        "    pp.pprint(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcUvtz5egaIy",
        "tags": []
      },
      "outputs": [],
      "source": [
        "inspect_examples(example_gen, num_examples=1, split_name='train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVmx7JHK8RkO"
      },
      "source": [
        "## StatisticsGen\n",
        "\n",
        "`StatisticsGen` component computes statistics over your dataset for data analysis, such as the number of examples, the number of features, and the data types of the features. It uses the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library. `StatisticsGen` takes as input the dataset we just ingested using `ExampleGen`.\n",
        "\n",
        "*Note that the statistics generator is appropriate for tabular data, and therefore, text dataset for this LLM tutorial may not be the optimal dataset for the analysis with statistics generator.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzeNGNEnyq_d",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from tfx.components import StatisticsGen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWWl7LeRKsXA",
        "tags": []
      },
      "outputs": [],
      "source": [
        "statistics_gen = tfx.components.StatisticsGen(\n",
        "    examples=example_gen.outputs['examples'], exclude_splits=['eval']\n",
        ")\n",
        "context.run(statistics_gen, enable_cache=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnWKjMyIVVB7"
      },
      "outputs": [],
      "source": [
        "context.show(statistics_gen.outputs['statistics'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqXFJyoO9O8-"
      },
      "source": [
        "## SchemaGen\n",
        "\n",
        "The `SchemaGen` component generates a schema based on your data statistics. (A schema defines the expected bounds, types, and properties of the features in your dataset.) It also uses the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library.\n",
        "\n",
        "Note: The generated schema is best-effort and only tries to infer basic properties of the data. It is expected that you review and modify it as needed.\n",
        "\n",
        "`SchemaGen` will take as input the statistics that we generated with `StatisticsGen`, looking at the training split by default.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpPFaV6tX5wQ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "schema_gen = tfx.components.SchemaGen(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    infer_feature_shape=False,\n",
        "    exclude_splits=['eval'],\n",
        ")\n",
        "context.run(schema_gen, enable_cache=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6DNNUi3YAmo",
        "tags": []
      },
      "outputs": [],
      "source": [
        "context.show(schema_gen.outputs['schema'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDdpADUb9VJR"
      },
      "source": [
        "## ExampleValidator\n",
        "\n",
        "The `ExampleValidator` component detects anomalies in your data, based on the expectations defined by the schema. It also uses the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library.\n",
        "\n",
        "`ExampleValidator` will take as input the statistics from `StatisticsGen`, and the schema from `SchemaGen`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_F5pLZ7YdZg"
      },
      "outputs": [],
      "source": [
        "example_validator = tfx.components.ExampleValidator(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    exclude_splits=['eval'],\n",
        ")\n",
        "context.run(example_validator, enable_cache=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After `ExampleValidator` finishes running, we can visualize the anomalies as a table."
      ],
      "metadata": {
        "id": "DgiXSTRawolF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eAHpc2UYfk_"
      },
      "outputs": [],
      "source": [
        "context.show(example_validator.outputs['anomalies'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H6fecGTiFmN"
      },
      "source": [
        "## Transform\n",
        "\n",
        "For a structured and repeatable design of a TFX pipeline we will need a scalable approach to feature engineering. The `Transform` component performs feature engineering for both training and serving. It uses the [TensorFlow Transform](https://www.tensorflow.org/tfx/transform/get_started) library.\n",
        "\n",
        "\n",
        "The Transform component uses a module file to supply user code for the feature engineering what we want to do, so our first step is to create that module file. We will only be working with the summary field.\n",
        "\n",
        "**Note:**\n",
        "*The %%writefile {_movies_transform_module_file} cell magic below creates and writes the contents of that cell to a file on the notebook server where this notebook is running (for example, the Colab VM). When doing this outside of a notebook you would just create a Python file.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22TBUtG9ME9N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.exists(\"modules\"):\n",
        "  os.mkdir(\"modules\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teaCGLgfnjw_"
      },
      "outputs": [],
      "source": [
        "_transform_module_file = 'modules/_transform_module.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rN6nRx3KnkpM"
      },
      "outputs": [],
      "source": [
        "%%writefile {_transform_module_file}\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def _fill_in_missing(x, default_value):\n",
        "  \"\"\"Replace missing values in a SparseTensor.\n",
        "\n",
        "  Fills in missing values of `x` with the default_value.\n",
        "\n",
        "  Args:\n",
        "    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n",
        "      in the second dimension.\n",
        "    default_value: the value with which to replace the missing values.\n",
        "\n",
        "  Returns:\n",
        "    A rank 1 tensor where missing values of `x` have been filled in.\n",
        "  \"\"\"\n",
        "  if not isinstance(x, tf.sparse.SparseTensor):\n",
        "    return x\n",
        "  return tf.squeeze(\n",
        "      tf.sparse.to_dense(\n",
        "          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n",
        "          default_value),\n",
        "      axis=1)\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "  outputs = {}\n",
        "  outputs[\"summary\"] = _fill_in_missing(inputs[\"summary\"],\"\")\n",
        "  return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-f5NaLTiFmO"
      },
      "outputs": [],
      "source": [
        "preprocessor = tfx.components.Transform(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    module_file=os.path.abspath(_transform_module_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkjIuwHeiFmO"
      },
      "outputs": [],
      "source": [
        "context.run(preprocessor, enable_cache=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH8OkaCwJgLF"
      },
      "source": [
        "Let's take a look at some of the transformed examples and check that they are indeed processed as intended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bt70Z16zJHy7"
      },
      "outputs": [],
      "source": [
        "def pprint_examples(artifact, n_examples=2):\n",
        "  print(\"artifact:\", artifact, \"\\n\")\n",
        "  uri = os.path.join(artifact.uri, \"Split-train\")\n",
        "  print(\"uri:\", uri, \"\\n\")\n",
        "  tfrecord_filenames = [os.path.join(uri, name) for name in os.listdir(uri)]\n",
        "  print(\"tfrecord_filenames:\", tfrecord_filenames, \"\\n\")\n",
        "  dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
        "  for tfrecord in dataset.take(n_examples):\n",
        "    serialized_example = tfrecord.numpy()\n",
        "    example = tf.train.Example.FromString(serialized_example)\n",
        "    pp.pprint(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg4I-TvXJIuO"
      },
      "outputs": [],
      "source": [
        "pprint_examples(preprocessor.outputs['transformed_examples'].get()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJll-vDn_eJP"
      },
      "source": [
        "## Trainer\n",
        "\n",
        "Trainer component trains an ML model, and it requires a model definition code from users.\n",
        "\n",
        "The `run_fn` function in TFX's Trainer component is the entry point for training a machine learning model. It is a user-supplied function that takes in a set of arguments and returns a model artifact.\n",
        "\n",
        "The `run_fn` function is responsible for:\n",
        "\n",
        "*   Building the machine learning model.\n",
        "*   Training the model on the training data.\n",
        "*   Saving the trained model to the serving model directory.\n",
        "\n",
        "\n",
        "### Write model training code\n",
        "We will create a very simple fine-tuned model, with the preprocessing GPT-2 model. First, we need to create a module that contains the run_fn function for TFX Trainer because TFX Trainer expects the run_fn function to be defined in a module.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQPtqKG5pmpn"
      },
      "outputs": [],
      "source": [
        "model_file = \"modules/model.py\"\n",
        "model_fn = \"modules.model.run_fn\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6drMNHJMAk7g"
      },
      "source": [
        "Now, we write the run_fn function:\n",
        "\n",
        "This run_fn function first gets the training data from the `fn_args.examples` argument. It then gets the schema of the training data from the `fn_args.schema` argument. Next, it loads finetuned GPT-2 model along with its preprocessor. The model is then trained on the training data using the model.train() method.\n",
        "Finally, the trained model weights are saved to the `fn_args.serving_model_dir` argument.\n",
        "\n",
        "\n",
        "Now, we are going to work with Keras NLP's GPT-2 Model! You can learn about the full GPT-2 model implementation in KerasNLP on [GitHub](https://github.com/keras-team/keras-nlp/blob/master/keras_nlp/models/gpt2/) or can read and interactively test the model on [Google IO2023 colab notebook](https://colab.research.google.com/github/tensorflow/codelabs/blob/main/KerasNLP/io2023_workshop.ipynb#scrollTo=81EZQ0D1R8LL ).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-ME_d8i2sTB"
      },
      "outputs": [],
      "source": [
        "import keras_nlp\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9yjLDqHoFb-"
      },
      "outputs": [],
      "source": [
        "%%writefile {model_file}\n",
        "\n",
        "import os\n",
        "import time\n",
        "from absl import logging\n",
        "import keras_nlp\n",
        "import more_itertools\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tfx\n",
        "import tfx.components.trainer.fn_args_utils\n",
        "import gc\n",
        "\n",
        "\n",
        "_EPOCH = 1\n",
        "_BATCH_SIZE = 20\n",
        "_INITIAL_LEARNING_RATE = 5e-5\n",
        "_END_LEARNING_RATE = 0.0\n",
        "\n",
        "\n",
        "def _input_fn(file_pattern: str) -\u003e list:\n",
        "  \"\"\"Retrieves training data and returns a list of articles for training.\n",
        "\n",
        "  For each row in the TFRecordDataset, generated in the previous ExampleGen\n",
        "  component, create a new tf.train.Example object and parse the TFRecord into\n",
        "  the example object. Articles, which are initially in bytes objects, are\n",
        "  decoded into a string.\n",
        "\n",
        "  Args:\n",
        "    file_pattern: Path to the TFRecord file of the training dataset.\n",
        "\n",
        "  Returns:\n",
        "    A list of training articles.\n",
        "\n",
        "  Raises:\n",
        "    FileNotFoundError: If TFRecord dataset is not found in the file_pattern\n",
        "    directory.\n",
        "  \"\"\"\n",
        "\n",
        "  if os.path.basename(file_pattern) == '*':\n",
        "    file_loc = os.path.dirname(file_pattern)\n",
        "\n",
        "  else:\n",
        "    raise FileNotFoundError(\n",
        "        f\"There is no file in the current directory: '{file_pattern}.\"\n",
        "    )\n",
        "\n",
        "  file_paths = [os.path.join(file_loc, name) for name in os.listdir(file_loc)]\n",
        "  train_articles = []\n",
        "  parsed_dataset = tf.data.TFRecordDataset(file_paths, compression_type=\"GZIP\")\n",
        "  for raw_record in parsed_dataset:\n",
        "    example = tf.train.Example()\n",
        "    example.ParseFromString(raw_record.numpy())\n",
        "    train_articles.append(\n",
        "        example.features.feature[\"summary\"].bytes_list.value[0].decode('utf-8')\n",
        "    )\n",
        "  return train_articles\n",
        "\n",
        "def run_fn(fn_args: tfx.components.trainer.fn_args_utils.FnArgs) -\u003e None:\n",
        "  \"\"\"Trains the model and outputs the trained model to a the desired location given by FnArgs.\n",
        "\n",
        "  Args:\n",
        "    FnArgs :  Args to pass to user defined training/tuning function(s)\n",
        "  \"\"\"\n",
        "\n",
        "  train_articles =  pd.Series(_input_fn(\n",
        "          fn_args.train_files[0],\n",
        "      ))\n",
        "  tf_train_ds = tf.data.Dataset.from_tensor_slices(train_articles)\n",
        "\n",
        "  gpt2_preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
        "      'gpt2_base_en',\n",
        "      sequence_length=256,\n",
        "      add_end_token=True,\n",
        "  )\n",
        "  gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n",
        "      'gpt2_base_en', preprocessor=gpt2_preprocessor\n",
        "  )\n",
        "\n",
        "  processed_ds = (\n",
        "      tf_train_ds.map(gpt2_preprocessor, tf.data.AUTOTUNE)\n",
        "      .batch(_BATCH_SIZE)\n",
        "      .cache()\n",
        "      .prefetch(tf.data.AUTOTUNE)\n",
        "  )\n",
        "\n",
        "  gpt2_lm.include_preprocessing = False\n",
        "\n",
        "  lr = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "      5e-5,\n",
        "      decay_steps=processed_ds.cardinality() * _EPOCH,\n",
        "      end_learning_rate=0.0,\n",
        "  )\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "  gpt2_lm.compile(\n",
        "      optimizer=keras.optimizers.experimental.Adam(lr),\n",
        "      loss=loss,\n",
        "      weighted_metrics=['accuracy'],\n",
        "  )\n",
        "\n",
        "  gpt2_lm.fit(processed_ds, epochs=_EPOCH)\n",
        "  if os.path.exists(fn_args.serving_model_dir):\n",
        "    os.rmdir(fn_args.serving_model_dir)\n",
        "  os.mkdir(fn_args.serving_model_dir)\n",
        "  gpt2_lm.backbone.save_weights(\n",
        "      filepath=os.path.join(fn_args.serving_model_dir, \"model_weights\"), save_format='tf'\n",
        "  )\n",
        "  del gpt2_lm, gpt2_preprocessor, processed_ds, tf_train_ds\n",
        "  gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnbMFKqc5gfK"
      },
      "outputs": [],
      "source": [
        "trainer = tfx.components.Trainer(\n",
        "    run_fn=model_fn,\n",
        "    examples=preprocessor.outputs['transformed_examples'],\n",
        "    train_args=tfx.proto.TrainArgs(splits=['train']),\n",
        "    eval_args=tfx.proto.EvalArgs(splits=['train']),\n",
        "    schema=schema_gen.outputs['schema'],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COCqeu-8CyHN"
      },
      "outputs": [],
      "source": [
        "context.run(trainer, enable_cache=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVeAWg28KmG0"
      },
      "source": [
        "## TFLite Converter\n",
        "\n",
        "We can convert the fine-tuned model into a TFLite model, which is a specific type of model that is designed to be used on mobile and embedded devices.\n",
        "TFLite models are smaller and faster than their TensorFlow counterparts, making them ideal for use on devices with limited resources.\n",
        "\n",
        "We can optimize the model using quantization. TensorFlow Lite can be used to shrink down the model size and accelerate model inference by mapping an input of continuous values to a discrete set.\n",
        "\n",
        "In this notebook you will use the [post-training dynamic range quantization](https://www.tensorflow.org/lite/performance/post_training_quant) by setting the converter optimization flag to 'tf.lite.Optimize.DEFAULT'. The rest of the conversion process is the same as before.\n",
        "\n",
        "If you are interested in how TFLite works and would like to interactively work with it on colab notebook, you can explore this [Google IO2023 Colab Notebook](https://colab.research.google.com/github/tensorflow/codelabs/blob/main/KerasNLP/io2023_workshop.ipynb#scrollTo=ZVrW3jyVNEH9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsZU7fM4LxJt"
      },
      "source": [
        "We will first create a TFLite artifact in order to save the TFLite model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mdbs7CirvTwA"
      },
      "outputs": [],
      "source": [
        "from tfx import types\n",
        "from tfx.types import artifact\n",
        "from enum import Enum\n",
        "\n",
        "Property = artifact.Property\n",
        "PropertyType = artifact.PropertyType\n",
        "\n",
        "\n",
        "DURATION_PROPERTY = Property(type=PropertyType.FLOAT)\n",
        "EPOCH_PROPERTY = Property(type=PropertyType.INT)\n",
        "SIZE_PROPERTY = Property(type=PropertyType.INT)\n",
        "QUANTIZED_PROPERTY = Property(type=PropertyType.STRING)\n",
        "\n",
        "class TFLite(types.Artifact):\n",
        "  \"\"\"Artifact that contains the trained model.\n",
        "\n",
        "  * Properties:\n",
        "    - `epoch`: The number of epochs it took to train the model.\n",
        "    - `model_size`: The size of the model\n",
        "    - `train_duration`: The total time elapsed for training and saving\n",
        "       the model in seconds.\n",
        "    - `is_quantized`: A boolean value indicating whether the model is the\n",
        "       quantized TFLite model or not.\n",
        "  \"\"\"\n",
        "\n",
        "  TYPE_NAME = 'TFLite'\n",
        "  PROPERTIES = {\n",
        "      'epoch': EPOCH_PROPERTY,\n",
        "      'model_size': SIZE_PROPERTY,\n",
        "      'conversion_duration': DURATION_PROPERTY,\n",
        "      'is_quantized': QUANTIZED_PROPERTY,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tw7jZQ2ATLV"
      },
      "source": [
        "We have to initialize the gpt-2 model to ensure that we are working with a base model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRgh6W3V5XoD"
      },
      "outputs": [],
      "source": [
        "gpt2_tokenizer = keras_nlp.models.GPT2Tokenizer.from_preset(\"gpt2_base_en\")\n",
        "gpt2_preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
        "    \"gpt2_base_en\",\n",
        "    sequence_length=256,\n",
        "    add_end_token=True,\n",
        ")\n",
        "gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\"gpt2_base_en\", preprocessor=gpt2_preprocessor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDVCfax9_7LF"
      },
      "source": [
        "First, you wrap the generate() function into a TensorFlow concrete function. The @tf.function decorator is used to convert a Python function to a TensorFlow graph function. This can improve the performance of the function by optimizing the TensorFlow operations that are executed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8KegzORlszj"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def generate(prompt, max_length):\n",
        "    return gpt2_lm.generate(prompt, max_length)\n",
        "\n",
        "concrete_func = generate.get_concrete_function(tf.TensorSpec([], tf.string), 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fVmWa-AvtnW"
      },
      "outputs": [],
      "source": [
        "\"\"\"A Converter Component transforms a trained model into TFLite Model.\n",
        "This is a Converter component in which the users can specify whether the model will\n",
        "be quantized or not.\n",
        "\"\"\"\n",
        "import time\n",
        "\n",
        "@tfx.dsl.components.component\n",
        "def Converter(\n",
        "    trained_model: tfx.dsl.components.InputArtifact[tfx.types.standard_artifacts.Model],\n",
        "    tflite_model: tfx.dsl.components.OutputArtifact[TFLite],\n",
        "    is_quantized: tfx.dsl.components.Parameter[bool]) -\u003e None:\n",
        "\n",
        "    # Creating a tflite model for generate function\n",
        "    trained_model_dir = os.path.join(trained_model.uri,'Format-Serving', 'model_weights')\n",
        "    gpt2_lm.load_weights(trained_model_dir)\n",
        "    gpt2_lm.jit_compile = False\n",
        "    generate_converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func],\n",
        "                                                            gpt2_lm)\n",
        "    generate_converter.target_spec.supported_ops = [\n",
        "        tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\n",
        "        tf.lite.OpsSet.SELECT_TF_OPS,  # enable TensorFlow ops.\n",
        "    ]\n",
        "    generate_converter.allow_custom_ops = True\n",
        "    if is_quantized:\n",
        "      generate_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "    generate_converter.target_spec.experimental_select_user_tf_ops = [\n",
        "        \"UnsortedSegmentJoin\",\n",
        "        \"UpperBound\",\n",
        "    ]\n",
        "    generate_converter._experimental_guarantee_all_funcs_one_use = True\n",
        "    start = time.time()\n",
        "    generate_tflite = generate_converter.convert()\n",
        "    end = time.time()\n",
        "    generate_path = os.path.join(tflite_model.uri, \"quantized_gpt2_generate.tflite\")\n",
        "    with open(generate_path, \"wb\") as f:\n",
        "      f.write(generate_tflite)\n",
        "    tflite_model.conversion_duration = end - start\n",
        "    tflite_model.model_size = os.path.getsize(generate_path)\n",
        "\n",
        "    # Creating a tflite model for Runner (needed for calculating perplexity)\n",
        "    runner_converter = tf.lite.TFLiteConverter.from_keras_model(gpt2_lm)\n",
        "    if is_quantized:\n",
        "      runner_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    runner_converter.target_spec.supported_ops = [\n",
        "      tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "      tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
        "    ]\n",
        "    runner_converter.allow_custom_ops = True\n",
        "    runner_converter.target_spec.experimental_select_user_tf_ops = [\"UnsortedSegmentJoin\", \"UpperBound\"]\n",
        "    runner_converter._experimental_guarantee_all_funcs_one_use = True\n",
        "\n",
        "    runner_tflite = runner_converter.convert()\n",
        "    runner_path = os.path.join(tflite_model.uri, \"quantized_gpt2_runner.tflite\")\n",
        "    with open(runner_path, \"wb\") as f:\n",
        "      f.write(runner_tflite)\n",
        "    if is_quantized:\n",
        "      tflite_model.is_quantized = \"QUANTIZED\"\n",
        "    else:\n",
        "      tflite_model.is_quantized = \"UNQUANTIZED\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISMl74Tyxjxf"
      },
      "source": [
        "Similarly, let's convert the trained model into a TFLite model by running the Conversion component with parameter 'is_quantized' specified to False."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpk788jnLdGX"
      },
      "outputs": [],
      "source": [
        "tflite_converter = Converter(trained_model = trainer.outputs['model'], is_quantized = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIshT5KMLhPG"
      },
      "outputs": [],
      "source": [
        "context.run(tflite_converter, enable_cache = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc76WRAmxXlu"
      },
      "source": [
        "Let's convert the trained model into a Quantized TFLite model by running the Conversion component with parameter 'is_quantized' specified to True."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHdd-kn1wVem"
      },
      "outputs": [],
      "source": [
        "quantized_converter = Converter(trained_model = trainer.outputs['model'], is_quantized = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkE9Jr5Z7Knu"
      },
      "outputs": [],
      "source": [
        "context.run(quantized_converter, enable_cache = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btljwhMwWeQ9"
      },
      "source": [
        "## Inference and Evaluation\n",
        "\n",
        "Now that we have fine-tuned the model and converted them to a TFlite model, let's test these models to make inferences. We will create a EvaluationMetric artifact since we want to save the inference results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S79afpeeVkwc"
      },
      "outputs": [],
      "source": [
        "from tfx.types import artifact\n",
        "from tfx import types\n",
        "\n",
        "Property = artifact.Property\n",
        "PropertyType = artifact.PropertyType\n",
        "\n",
        "DURATION_PROPERTY = Property(type=PropertyType.FLOAT)\n",
        "EVAL_OUTPUT_PROPERTY = Property(type=PropertyType.STRING)\n",
        "\n",
        "class EvaluationMetric(types.Artifact):\n",
        "  \"\"\"Artifact that contains metrics for a model.\n",
        "\n",
        "  * Properties:\n",
        "\n",
        "     - 'model_prediction_time' : time it took for the model to make predictions\n",
        "     based on the input text.\n",
        "     - 'model_evaluation_output_path' : saves the path to the CSV file that\n",
        "     contains the model's prediction based on the testing inputs.\n",
        "  \"\"\"\n",
        "  TYPE_NAME = 'Evaluation_Metric'\n",
        "  PROPERTIES = {\n",
        "      'model_prediction_time': DURATION_PROPERTY,\n",
        "      'model_evaluation_output_path': EVAL_OUTPUT_PROPERTY,\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg7X_S1lAL7b"
      },
      "source": [
        "Here, we are defining a helper function to run inference with a given input and a TFLite model. Note that here TensorFlow Text ops are registered as custom ops so that the interpreter can run smoothly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSjmByAAM2OI"
      },
      "outputs": [],
      "source": [
        "from tensorflow.lite.python import interpreter\n",
        "import tensorflow_text as tf_text\n",
        "def run_inference(input, generator):\n",
        "  output = generator(prompt=np.array([input]))\n",
        "  output_text = output[\"output_0\"].item(0).decode('utf-8')\n",
        "  return output_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ8qyPV8cMZI"
      },
      "source": [
        "In order to produce logits with TFLite models, we need to create a separate interpreter such that takes in a TFLite model as input and returns an object defined as \"runner\" that we need later for comupting perplexity for tflite models.\n",
        "\n",
        "\n",
        "The function first creates an interpreter.InterpreterWithCustomOps object, which is a specialized interpreter that can run TFLite models that use custom TensorFlow ops. The interpreter.InterpreterWithCustomOps object is then configured to use the TFText ops.\n",
        "\n",
        "Next, the function resizes the input tensors to the specified size. This is necessary because the default size of input tensors in TFLite models is [1,1]. Finally, the function allocates the tensors in the interpreter and gets the signature list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGY9_xSywMVz"
      },
      "outputs": [],
      "source": [
        "from tensorflow.lite.python import interpreter\n",
        "def create_runner(tflite_model):\n",
        "  interp = interpreter.InterpreterWithCustomOps(\n",
        "      model_content=tflite_model,\n",
        "      custom_op_registerers=tf_text.tflite_registrar.SELECT_TFTEXT_OPS)\n",
        "\n",
        "  # Need to resize tensor inputs or they will be [1,1]!\n",
        "  interp.resize_tensor_input(0, (1,255))\n",
        "  interp.resize_tensor_input(1, (1,255))\n",
        "  interp.allocate_tensors()\n",
        "  interp.get_signature_list()\n",
        "  runner = interp.get_signature_runner('serving_default')\n",
        "  return runner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ3Wq2Ylb6JF"
      },
      "source": [
        "Below are some helper functions for calculating perplexity, a measure of how well a language model predicts the next word in a sequence. It is calculated by taking the exponential of the negative log likelihood of the observed sequence. A lower perplexity score indicates that the model is more confident in its predictions, which makes it more likely that the model will perform better.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkXaZlsg38jI"
      },
      "outputs": [],
      "source": [
        "\"\"\"This is an evaluation component for the LLM pipeline takes in a\n",
        "standard trainer artifact and outputs a custom evaluation artifact.\n",
        "It displays the evaluation output in the colab notebook.\n",
        "\"\"\"\n",
        "import os\n",
        "import time\n",
        "import keras_nlp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tfx.v1 as tfx\n",
        "\n",
        "_TEST_SIZE = 2\n",
        "_INPUT_LENGTH = 25\n",
        "\n",
        "def input_fn(file_pattern: str) -\u003e list:\n",
        "  \"\"\"Retrieves training data and returns a list of articles for training.\n",
        "\n",
        "  Args:\n",
        "    file_pattern: Path to the TFRecord file of the training dataset.\n",
        "\n",
        "  Returns:\n",
        "    A list of test articles\n",
        "\n",
        "  Raises:\n",
        "    FileNotFoundError: If the file path does not exist.\n",
        "  \"\"\"\n",
        "  if os.path.exists(file_pattern):\n",
        "    file_paths = [os.path.join(file_pattern, name) for name in os.listdir(file_pattern)]\n",
        "    test_articles = []\n",
        "    parsed_dataset = tf.data.TFRecordDataset(file_paths, compression_type=\"GZIP\")\n",
        "    for raw_record in parsed_dataset:\n",
        "      example = tf.train.Example()\n",
        "      example.ParseFromString(raw_record.numpy())\n",
        "      test_articles.append(\n",
        "          example.features.feature[\"summary\"].bytes_list.value[0].decode('utf-8')\n",
        "      )\n",
        "    return test_articles\n",
        "  else:\n",
        "    raise FileNotFoundError(f'File path \"{file_pattern}\" does not exist.')\n",
        "\n",
        "def trim_sentence(sentence: str, max_words: int = 20):\n",
        "  \"\"\"Trims the sentence to include up to the given number of words.\n",
        "\n",
        "  Args:\n",
        "    sentence: The sentence to trim.\n",
        "    max_words: The maximum number of words to include in the trimmed sentence.\n",
        "\n",
        "  Returns:\n",
        "    The trimmed sentence.\n",
        "  \"\"\"\n",
        "  words = sentence.split(' ')\n",
        "  if len(words) \u003c= max_words:\n",
        "    return sentence\n",
        "  return ' '.join(words[:max_words])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypRrAQMpfEFd"
      },
      "source": [
        "![perplexity.png](images/gpt2_fine_tuning_and_conversion/perplexity.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo5fvOa9GmzL"
      },
      "source": [
        "One of the useful metrics for evaluating a Large Language Model is **Perplexity**. Perplexity is a measure of how well a language model predicts the next token in a sequence. It is calculated by taking the exponentiation of the average negative log-likelihood of the next token. A lower perplexity score indicates that the language model is better at predicting the next token.\n",
        "\n",
        "This is the *formula* for calculating perplexity.\n",
        "\n",
        " $\\text{Perplexity} = \\exp(-1 * $ Average Negative Log Likelihood $) =\n",
        "  \\exp\\left(-\\frac{1}{T} \\sum_{t=1}^T \\log p(w_t | w_{\u003ct})\\right)$.\n",
        "\n",
        "\n",
        "In this colab notebook, we calculate perplexity using [keras_nlp's perplexity](https://keras.io/api/keras_nlp/metrics/perplexity/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNfs9ZplgPAH"
      },
      "source": [
        "**Computing Perplexity for Base GPT-2 Model and Finetuned Model**\n",
        "\n",
        "The code below is the function which will be used later in the notebook for computing perplexity for the base GPT-2 model and the finetuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27iA8w6-GlSz"
      },
      "outputs": [],
      "source": [
        "def calculate_perplexity(gpt2_model, gpt2_tokenizer, sentence) -\u003e int:\n",
        "  \"\"\"Calculates perplexity of a model given a sentence.\n",
        "\n",
        "  Args:\n",
        "    gpt2_model: GPT-2 Language Model\n",
        "    gpt2_tokenizer: A GPT-2 tokenizer using Byte-Pair Encoding subword segmentation.\n",
        "    sentence: Sentence that the model's perplexity is calculated upon.\n",
        "\n",
        "  Returns:\n",
        "    A perplexity score.\n",
        "  \"\"\"\n",
        "  # gpt2_tokenizer([sentence])[0] produces a tensor containing an array of tokens that form the sentence.\n",
        "  tokens = gpt2_tokenizer([sentence])[0].numpy()\n",
        "  # decoded_sentences is an array containing sentences that increase by one token in size.\n",
        "  # e.g. if tokens for a sentence \"I love dogs\" are [\"I\", \"love\", \"dogs\"], then decoded_sentences = [\"I love\", \"I love dogs\"]\n",
        "  decoded_sentences = [gpt2_tokenizer.detokenize([tokens[:i]])[0].numpy() for i in range(1, len(tokens))]\n",
        "  predictions = gpt2_model.predict(decoded_sentences)\n",
        "  logits = [predictions[i - 1][i] for i in range(1, len(tokens))]\n",
        "  target = tokens[1:].reshape(len(tokens) - 1, 1)\n",
        "  perplexity = keras_nlp.metrics.Perplexity(from_logits=True)\n",
        "  perplexity.update_state(target, logits)\n",
        "  result = perplexity.result()\n",
        "  return result.numpy()\n",
        "\n",
        "def average_perplexity(gpt2_model, gpt2_tokenizer, sentences):\n",
        "  perplexity_lst = [calculate_perplexity(gpt2_model, gpt2_tokenizer, sent) for sent in sentences]\n",
        "  return np.mean(perplexity_lst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yUSevqBdCiA"
      },
      "source": [
        "**Computing Perplexity for TFLite Models**\n",
        "\n",
        "Similar to the code above for calculating perplexity for the base model and finetuned GPT-2 model, we also write a function that computes perplexity for TFlite models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fisabLslwx2f"
      },
      "outputs": [],
      "source": [
        "def calculate_perplexity_tflite(sentence, runner, gpt2_tokenizer, gpt2_preprocessor):\n",
        "    \"\"\"Calculates perplexity of a TfLite model given a sentence.\n",
        "\n",
        "    Args:\n",
        "      sentence: Sentence that the model's perplexity is calculated upon.\n",
        "      runner: Object that is used to run the TensorFlow model's serving_default signature.\n",
        "      gpt2_tokenizer: A GPT-2 tokenizer using Byte-Pair Encoding subword segmentation.\n",
        "      gpt2_preprocessor: GPT2 preprocessing layer which tokenizes and packs inputs.\n",
        "\n",
        "    Returns:\n",
        "      A perplexity score.\n",
        "    \"\"\"\n",
        "    # Tokenize and preprocess the sentence\n",
        "    tokenized_sentence = gpt2_tokenizer([sentence])[0].numpy()\n",
        "    ds = tf.data.Dataset.from_tensor_slices([sentence])\n",
        "    preprocessed_ds = ds.map(gpt2_preprocessor)\n",
        "    _, token_ids, padding_mask = next(iter(preprocessed_ds))\n",
        "\n",
        "    # Run the TensorFlow Lite model\n",
        "    tflite_result = runner(token_ids=tf.expand_dims(token_ids, axis=0), padding_mask=tf.expand_dims(tf.cast(padding_mask, tf.int32), axis=0))\n",
        "    prediction_key = list(tflite_result.keys())[0]\n",
        "    tflite_prediction = tflite_result[prediction_key]\n",
        "\n",
        "    # Extract logits for all tokens\n",
        "    logits = tflite_prediction[0][1:len(tokenized_sentence)]\n",
        "\n",
        "    # Target tokens excluding the first token\n",
        "    target = tokenized_sentence[1:].reshape(len(tokenized_sentence) - 1, 1)\n",
        "\n",
        "    # Calculate perplexity\n",
        "    perplexity = keras_nlp.metrics.Perplexity(from_logits=True)\n",
        "    perplexity.update_state(target, logits)\n",
        "    result = perplexity.result()\n",
        "\n",
        "    return result.numpy()\n",
        "\n",
        "def average_perplexity_tflite(sentences, runner):\n",
        "    gpt2_tokenizer = keras_nlp.models.GPT2Tokenizer.from_preset(\"gpt2_base_en\")\n",
        "    gpt2_preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
        "        \"gpt2_base_en\",\n",
        "        sequence_length=256,\n",
        "        add_end_token=True,\n",
        "    )\n",
        "    perplexity_lst = [calculate_perplexity_tflite(sent, runner, gpt2_tokenizer, gpt2_preprocessor) for sent in sentences]\n",
        "    return np.mean(perplexity_lst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELmkaY-ygbog"
      },
      "source": [
        "## Evaluator\n",
        "\n",
        "Now that we have all the helper functions required for the Evaluator component, we will define a evaluator component. Evaluator component makes inferences with base, fine-tuned, TFlite, and quantized models, calculates perplexity for all models, and also records inference time. Output of the evaluator component contains information useful for assessing each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eb5fD5vzEQJ0"
      },
      "outputs": [],
      "source": [
        "@tfx.dsl.components.component\n",
        "def Evaluator(\n",
        "    examples: tfx.dsl.components.InputArtifact[\n",
        "        tfx.types.standard_artifacts.Examples\n",
        "    ],\n",
        "    trained_model: tfx.dsl.components.InputArtifact[\n",
        "        tfx.types.standard_artifacts.Model\n",
        "    ],\n",
        "    tflite_model: tfx.dsl.components.InputArtifact[TFLite],\n",
        "    quantized_model: tfx.dsl.components.InputArtifact[TFLite],\n",
        "    max_length: tfx.dsl.components.Parameter[int],\n",
        "    evaluation: tfx.dsl.components.OutputArtifact[EvaluationMetric],\n",
        ") -\u003e None:\n",
        "  \"\"\"Makes inferences with base model, finetuned model, TFlite model, and quantized model.\n",
        "\n",
        "  Args:\n",
        "    examples: Standard TFX examples artifacts for retreiving test dataset.\n",
        "    trained_model: Standard TFX trained model artifact finetuned with Multi-News\n",
        "      dataset.\n",
        "    tflite_model: Unquantized TFLite model.\n",
        "    quantized_model: Quantized TFLite model.\n",
        "    max_length: Length of the text that the model generates given custom input\n",
        "      statements.\n",
        "    evaluation: An evaluation artifact that saves predicted outcomes of custom\n",
        "      inputs in a csv document and inference speed of the model.\n",
        "  \"\"\"\n",
        "  _TEST_SIZE = 10\n",
        "\n",
        "  path = os.path.join(examples.uri, 'Split-eval')\n",
        "  test_data = input_fn(path)\n",
        "  evaluation_inputs = [\n",
        "      trim_sentence(article, max_words=_INPUT_LENGTH)\n",
        "      for article in test_data[:_TEST_SIZE]\n",
        "  ]\n",
        "  true_test = [\n",
        "      trim_sentence(article, max_words=max_length)\n",
        "      for article in test_data[:_TEST_SIZE]\n",
        "  ]\n",
        "\n",
        "  # Loading base model, making inference, and calculating perplexity on the base model.\n",
        "  gpt2_preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
        "      'gpt2_base_en',\n",
        "      sequence_length=256,\n",
        "      add_end_token=True,\n",
        "  )\n",
        "  gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n",
        "      'gpt2_base_en', preprocessor=gpt2_preprocessor\n",
        "  )\n",
        "  gpt2_tokenizer = keras_nlp.models.GPT2Tokenizer.from_preset('gpt2_base_en')\n",
        "\n",
        "  base_average_perplexity = average_perplexity(\n",
        "      gpt2_lm, gpt2_tokenizer, true_test\n",
        "  )\n",
        "\n",
        "  start_base_model = time.time()\n",
        "  base_evaluation = [\n",
        "      gpt2_lm.generate(input, max_length).numpy().decode('utf-8')\n",
        "      for input in evaluation_inputs\n",
        "  ]\n",
        "  end_base_model = time.time()\n",
        "\n",
        "  # Loading finetuned model and making inferences with the finetuned model.\n",
        "  model_weights = os.path.join(\n",
        "      trained_model.uri, 'Format-Serving', 'model_weights'\n",
        "  )\n",
        "  gpt2_lm.load_weights(model_weights)\n",
        "\n",
        "  trained_model_average_perplexity = average_perplexity(\n",
        "      gpt2_lm, gpt2_tokenizer, true_test\n",
        "  )\n",
        "\n",
        "  start_trained = time.time()\n",
        "  trained_evaluation = [\n",
        "      gpt2_lm.generate(input, max_length).numpy().decode('utf-8')\n",
        "      for input in evaluation_inputs\n",
        "  ]\n",
        "  end_trained = time.time()\n",
        "\n",
        "  from tensorflow.lite.python import interpreter\n",
        "  # Loading TFLite model and making inference with TFLite model.\n",
        "  with open(\n",
        "      os.path.join(tflite_model.uri, 'quantized_gpt2_generate.tflite'), 'rb'\n",
        "  ) as generate_file:\n",
        "    tflite_model_generate = generate_file.read()\n",
        "  with open(\n",
        "      os.path.join(tflite_model.uri, 'quantized_gpt2_runner.tflite'), 'rb'\n",
        "  ) as runner_file:\n",
        "    tflite_model_runner_file = runner_file.read()\n",
        "  tflite_model_runner = create_runner(tflite_model_runner_file)\n",
        "\n",
        "  tflite_model_average_perplexity = average_perplexity_tflite(\n",
        "      true_test, tflite_model_runner\n",
        "  )\n",
        "\n",
        "  interp = interpreter.InterpreterWithCustomOps(\n",
        "      model_content=tflite_model_generate,\n",
        "      custom_op_registerers=tf_text.tflite_registrar.SELECT_TFTEXT_OPS)\n",
        "  interp.get_signature_list()\n",
        "  generator = interp.get_signature_runner('serving_default')\n",
        "\n",
        "  start_tflite = time.time()\n",
        "  tflite_evaluation = [\n",
        "      run_inference(input, generator) for input in evaluation_inputs\n",
        "  ]\n",
        "  end_tflite = time.time()\n",
        "\n",
        "  # Loading quantized model and making inference with Quantized model.\n",
        "  with open(\n",
        "      os.path.join(quantized_model.uri, 'quantized_gpt2_generate.tflite'), 'rb'\n",
        "  ) as generate_file:\n",
        "    quantized_model_generate = generate_file.read()\n",
        "  with open(\n",
        "      os.path.join(quantized_model.uri, 'quantized_gpt2_runner.tflite'), 'rb'\n",
        "  ) as runner_file:\n",
        "    quantized_model_runner_file = runner_file.read()\n",
        "  quantized_model_runner = create_runner(quantized_model_runner_file)\n",
        "\n",
        "  quantized_model_average_perplexity = average_perplexity_tflite(\n",
        "      true_test, quantized_model_runner\n",
        "  )\n",
        "\n",
        "  interp = interpreter.InterpreterWithCustomOps(\n",
        "      model_content=quantized_model_generate,\n",
        "      custom_op_registerers=tf_text.tflite_registrar.SELECT_TFTEXT_OPS)\n",
        "  interp.get_signature_list()\n",
        "  generator = interp.get_signature_runner('serving_default')\n",
        "\n",
        "  start_quantized = time.time()\n",
        "  quantized_evaluation = [\n",
        "      run_inference(input, generator)\n",
        "      for input in evaluation_inputs\n",
        "  ]\n",
        "  end_quantized = time.time()\n",
        "\n",
        "  # Building an inference table.\n",
        "  inference_data = {\n",
        "      'input': evaluation_inputs,\n",
        "      'actual_test_output': true_test,\n",
        "      'base_model_prediction': base_evaluation,\n",
        "      'trained_model_prediction': trained_evaluation,\n",
        "      'tflite_evaluation_prediction': tflite_evaluation,\n",
        "      'quantized_model_prediction': quantized_evaluation,\n",
        "  }\n",
        "\n",
        "  models = [\n",
        "      'Base Model',\n",
        "      'Finetuned Model',\n",
        "      'TFLite Model',\n",
        "      'Quantized Model',\n",
        "  ]\n",
        "  inference_time = [\n",
        "      (end_base_model - start_base_model),\n",
        "      (end_trained - start_trained),\n",
        "      (end_tflite - start_tflite),\n",
        "      (end_quantized - start_quantized),\n",
        "  ]\n",
        "  average_inference_time = [time / _TEST_SIZE for time in inference_time]\n",
        "  average_perplexity_lst = [\n",
        "      base_average_perplexity,\n",
        "      trained_model_average_perplexity,\n",
        "      tflite_model_average_perplexity,\n",
        "      quantized_model_average_perplexity,\n",
        "  ]\n",
        "  evaluation_data = {\n",
        "      'Model': models,\n",
        "      'Average Inference Time (sec)': average_inference_time,\n",
        "      'Average Perplexity': average_perplexity_lst,\n",
        "  }\n",
        "\n",
        "  # creating directory in examples artifact to save metric dataframes\n",
        "  metrics_path = os.path.join(evaluation.uri, 'metrics')\n",
        "  if not os.path.exists(metrics_path):\n",
        "      os.mkdir(metrics_path)\n",
        "\n",
        "  evaluation_df = pd.DataFrame(evaluation_data).set_index('Model').transpose()\n",
        "  evaluation_path = os.path.join(metrics_path, 'evaluation_output.csv')\n",
        "  evaluation_df.to_csv(evaluation_path)\n",
        "\n",
        "  inference_df = pd.DataFrame(inference_data)\n",
        "  inference_path = os.path.join(metrics_path, 'inference_output.csv')\n",
        "  inference_df.to_csv(inference_path)\n",
        "  evaluation.model_evaluation_output_path = inference_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkC0RrleWP9O"
      },
      "outputs": [],
      "source": [
        "evaluator = Evaluator(examples = example_gen.outputs['examples'],\n",
        "                      trained_model = trainer.outputs['model'],\n",
        "                      quantized_model = quantized_converter.outputs['tflite_model'],\n",
        "                      tflite_model = tflite_converter.outputs['tflite_model'],\n",
        "                      max_length = 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQQvbT96XXDT"
      },
      "outputs": [],
      "source": [
        "context.run(evaluator, enable_cache = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluator Results"
      ],
      "metadata": {
        "id": "xVUIimCogdjZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once our evaluation component execution is completed, we will load the evaluation metrics from evaluator URI and display them.\n",
        "\n",
        "\n",
        "*Note:*\n",
        "\n",
        "**Perplexity Calculation:**\n",
        "*Perplexity is only one of many ways to evaluate LLMs. LLM evaluation is an [active research topic](https://arxiv.org/abs/2307.03109) and a comprehensive treatment is beyond the scope of this notebook.*\n",
        "\n",
        "**Quantized TFLite model inference time:**\n",
        "*Inference time of quantized models should be faster than baseline model and we observed the quantized TFLite model results to be slower than expected .This is because [quantized int relies on special instructions that have not been emphasized on x86_64 architecture](https://github.com/tensorflow/tensorflow/issues/21698).*"
      ],
      "metadata": {
        "id": "EPKArU8f3FpD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVv5F_Ok7Jss"
      },
      "outputs": [],
      "source": [
        "evaluation_path = os.path.join(evaluator.outputs['evaluation']._artifacts[0].uri, 'metrics')\n",
        "inference_df = pd.read_csv(os.path.join(evaluation_path, 'inference_output.csv'), index_col=0)\n",
        "evaluation_df = pd.read_csv(os.path.join(evaluation_path, 'evaluation_output.csv'), index_col=0)"
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "Perplexity calculations for GPT-2 model looks deviated as model.predict() for GPT2CasualLM is broken in the keras-nlp IO-2023 version. This can be fixed by using latest version of keras-nlp module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvtAnvrm6H-a"
      },
      "outputs": [],
      "source": [
        "from IPython import display\n",
        "display.display(display.HTML(inference_df.to_html()))\n",
        "display.display(display.HTML(evaluation_df.to_html()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiCy6OQ7J3C5"
      },
      "source": [
        "## Running the Entire Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvYtjmkFHSxu"
      },
      "source": [
        "TFX supports multiple orchestrators to run pipelines. In this tutorial we will use LocalDagRunner which is included in the TFX Python package and runs pipelines on local environment. We often call TFX pipelines \"DAGs\" which stands for directed acyclic graph.\n",
        "\n",
        "LocalDagRunner provides fast iterations for development and debugging. TFX also supports other orchestrators including Kubeflow Pipelines and Apache Airflow which are suitable for production use cases. See [TFX on Cloud AI Platform Pipelines](https://www.tensorflow.org/tfx/tutorials/tfx/cloud-ai-platform-pipelines) or [TFX Airflow](https://www.tensorflow.org/tfx/tutorials/tfx/airflow_workshop) Tutorial to learn more about other orchestration systems.\n",
        "\n",
        "Now we create a LocalDagRunner and pass a Pipeline object created from the function we already defined. The pipeline runs directly and you can see logs for the progress of the pipeline including ML model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FQgyxOQLn22"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "PIPELINE_NAME = \"tfx-llm-multi-news\"\n",
        "model_fn = \"modules.model.run_fn\"\n",
        "_transform_module_file = \"modules/_transform_module.py\"\n",
        "\n",
        "# Output directory to store artifacts generated from the pipeline.\n",
        "PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n",
        "# Path to a SQLite DB file to use as an MLMD storage.\n",
        "METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n",
        "# Output directory where created models from the pipeline will be exported.\n",
        "SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n",
        "\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.INFO)  # Set default logging level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8PG2V3RTrdG"
      },
      "source": [
        "**NOTE:** If you would like to create multiple objects from the same component (as in the case of creating `tflite_converter` and `quantized_converter` from `Converter` component in this pipeline), you need to specify the object by '.with_id()'. An example of this practice is in the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgTwBpN-pe3_"
      },
      "outputs": [],
      "source": [
        "def _create_pipeline(\n",
        "    pipeline_name: str,\n",
        "    pipeline_root: str,\n",
        "    model_fn: str,\n",
        "    serving_model_dir: str,\n",
        "    metadata_path: str,\n",
        ") -\u003e tfx.dsl.Pipeline:\n",
        "  \"\"\"Creates a Pipeline for Fine-Tuning and Converting an Large Language Model with TFX.\"\"\"\n",
        "\n",
        "  example_gen = FileBasedExampleGen(\n",
        "    input_base='dummy',\n",
        "    custom_config={'dataset':'multi_news', 'split':'train[:20%]'},\n",
        "    custom_executor_spec=executor_spec.BeamExecutorSpec(TFDSExecutor))\n",
        "\n",
        "  statistics_gen = tfx.components.StatisticsGen(\n",
        "      examples=example_gen.outputs['examples'], exclude_splits=['eval']\n",
        "  )\n",
        "\n",
        "  schema_gen = tfx.components.SchemaGen(\n",
        "      statistics=statistics_gen.outputs['statistics'],\n",
        "      infer_feature_shape=False,\n",
        "      exclude_splits=['eval'],\n",
        "  )\n",
        "\n",
        "  example_validator = tfx.components.ExampleValidator(\n",
        "      statistics=statistics_gen.outputs['statistics'],\n",
        "      schema=schema_gen.outputs['schema'],\n",
        "      exclude_splits=['eval'],\n",
        "  )\n",
        "\n",
        "  preprocessor = tfx.components.Transform(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    module_file= _transform_module_file,\n",
        "  )\n",
        "\n",
        "  trainer = tfx.components.Trainer(\n",
        "      run_fn=model_fn,\n",
        "      examples=example_gen.outputs['examples'],\n",
        "      train_args=tfx.proto.TrainArgs(splits=['train']),\n",
        "      eval_args=tfx.proto.EvalArgs(splits=['train']),\n",
        "      schema=schema_gen.outputs['schema'],\n",
        "  )\n",
        "\n",
        "  quantized_converter = Converter(\n",
        "      trained_model=trainer.outputs['model'], is_quantized=True\n",
        "  ).with_id('quantized model')\n",
        "\n",
        "  tflite_converter = Converter(\n",
        "      trained_model=trainer.outputs['model'], is_quantized=False\n",
        "  ).with_id('tflite model')\n",
        "\n",
        "  evaluator = Evaluator(\n",
        "      examples=example_gen.outputs['examples'],\n",
        "      trained_model=trainer.outputs['model'],\n",
        "      quantized_model=quantized_converter.outputs['tflite_model'],\n",
        "      tflite_model=tflite_converter.outputs['tflite_model'],\n",
        "      max_length=100,\n",
        "  )\n",
        "\n",
        "  # Following 9 components will be included in the pipeline.\n",
        "  components = [\n",
        "      example_gen,\n",
        "      statistics_gen,\n",
        "      schema_gen,\n",
        "      example_validator,\n",
        "      preprocessor,\n",
        "      trainer,\n",
        "      quantized_converter,\n",
        "      tflite_converter,\n",
        "      evaluator,\n",
        "  ]\n",
        "\n",
        "  return tfx.dsl.Pipeline(\n",
        "      pipeline_name=pipeline_name,\n",
        "      pipeline_root=pipeline_root,\n",
        "      metadata_connection_config=tfx.orchestration.metadata.sqlite_metadata_connection_config(\n",
        "          metadata_path\n",
        "      ),\n",
        "      components=components,\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkgLXyZGJ9CO"
      },
      "outputs": [],
      "source": [
        "tfx.orchestration.LocalDagRunner().run(\n",
        "    _create_pipeline(\n",
        "        pipeline_name=PIPELINE_NAME,\n",
        "        pipeline_root=PIPELINE_ROOT,\n",
        "        model_fn=model_fn,\n",
        "        serving_model_dir=SERVING_MODEL_DIR,\n",
        "        metadata_path=METADATA_PATH,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo3Z08xzHa4G"
      },
      "source": [
        "You should see INFO:absl:Component Evaluation is finished.\" at the end of the logs if the pipeline finished successfully because evaluation component is the last component of the pipeline."
      ]
    }
  ]
}
